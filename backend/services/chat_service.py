from typing import Tuple, List, Dict
from backend.core.config import settings
from backend.core.session import append_message, get_history
from backend.nlp.openai_client import chat_with_llm

def _trim_history_for_llm(history: List[Dict[str, str]], max_turns: int) -> List[Dict[str, str]]:
    """
    Keep only the last N turns (user+assistant messages).
    Each turn ~ 2 messages. So we keep last max_turns*2 messages.
    """
    keep = max_turns * 2
    return history[-keep:] if len(history) > keep else history

def chat_reply_dummy(session_id: str, message: str) -> Tuple[str, int, str]:
    msg = message.strip()
    if not msg:
        return "", 0, "Invalid input (empty message)."

    append_message(session_id, "user", msg)

    if settings.use_llm_chat:
        history = get_history(session_id)
        history_for_llm = _trim_history_for_llm(history, settings.chat_max_turns)

        system = {
            "role": "system",
            "content": (
                "You are an NLP Showcase assistant. Be concise, helpful, and professional. "
                "If a request is unclear, ask one short clarifying question."
            ),
        }

        llm_input = [system] + history_for_llm

        reply = chat_with_llm(llm_input)
        note = "LLM chat: response generated by OpenAI model."
    else:
        low = msg.lower()
        if any(w in low for w in ["hello", "hi", "hey"]):
            reply = "Hello. I am your NLP Showcase assistant. How can I help you?"
        else:
            reply = f"I received your message: '{msg}'. (Dummy chat; real LLM later.)"
        note = "Dummy chat: in-memory session + rule-based reply."

    append_message(session_id, "assistant", reply)

    session_size = len(get_history(session_id))
    return reply, session_size, note
